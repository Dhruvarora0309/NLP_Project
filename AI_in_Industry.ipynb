{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38cf219f2e7844f1b58f3c919f9e100f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e7f4a133caa4c318ead64102d2644f7",
              "IPY_MODEL_186773a73f09468eb5cc5b940aee9a34",
              "IPY_MODEL_7e0836aa68c34f0988c25940f2d048ed"
            ],
            "layout": "IPY_MODEL_c6e913e01f774674a4e237d30464ce2f"
          }
        },
        "4e7f4a133caa4c318ead64102d2644f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd2026efc17843c795a3dbc28f560205",
            "placeholder": "​",
            "style": "IPY_MODEL_1b8e13f6d87740818e0ea4eaddb00775",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "186773a73f09468eb5cc5b940aee9a34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4d832a8994947089d10dfcff63d971a",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5df328136ff4aa29f349e46be82d6bf",
            "value": 5
          }
        },
        "7e0836aa68c34f0988c25940f2d048ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa1274caed7b4c13834bba4b33f16958",
            "placeholder": "​",
            "style": "IPY_MODEL_0ccf856bde8a4634b03da0c270684a89",
            "value": " 5/5 [00:01&lt;00:00,  2.87it/s]"
          }
        },
        "c6e913e01f774674a4e237d30464ce2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd2026efc17843c795a3dbc28f560205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8e13f6d87740818e0ea4eaddb00775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4d832a8994947089d10dfcff63d971a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5df328136ff4aa29f349e46be82d6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa1274caed7b4c13834bba4b33f16958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ccf856bde8a4634b03da0c270684a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b67a3e75f3b24b268e62adcb060ec224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77bd77175a9e4f5da434bec800ca059e",
              "IPY_MODEL_122fd598de224df3aa65f014c40122d8",
              "IPY_MODEL_0ca3784e4bb14310a129aee151d387e4"
            ],
            "layout": "IPY_MODEL_da7e72baf30b4e4095b674877b318de0"
          }
        },
        "77bd77175a9e4f5da434bec800ca059e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_569f0d5a6b6a4c7bb21efb4dd6cfd2e9",
            "placeholder": "​",
            "style": "IPY_MODEL_aadd3593520d4b249fa1e07305a4ad66",
            "value": "100%"
          }
        },
        "122fd598de224df3aa65f014c40122d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0275213b87984070b0cd5ef64dde7981",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0e5a3b15d864ab0a13e6b3a81be0a85",
            "value": 50
          }
        },
        "0ca3784e4bb14310a129aee151d387e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a47e90e43d5c44dda2e3d8b6fe4f645d",
            "placeholder": "​",
            "style": "IPY_MODEL_0999a45b51a24d789d7cc3b710e4aca7",
            "value": " 50/50 [02:46&lt;00:00,  3.04s/it]"
          }
        },
        "da7e72baf30b4e4095b674877b318de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569f0d5a6b6a4c7bb21efb4dd6cfd2e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aadd3593520d4b249fa1e07305a4ad66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0275213b87984070b0cd5ef64dde7981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0e5a3b15d864ab0a13e6b3a81be0a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a47e90e43d5c44dda2e3d8b6fe4f645d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0999a45b51a24d789d7cc3b710e4aca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhruvarora0309/NLP_Project/blob/main/AI_in_Industry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o43sEoxhP-6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install moviepy\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "Z1vefpB4P_dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "xv3OhbHuP9PD",
        "outputId": "c7aefabb-ee51-477a-a8d3-a3da75f2e25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://542e66d0b04929a775.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://542e66d0b04929a775.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# CAPTION GENERATOR\n",
        "\n",
        "# Step 1: Extract frames from the video\n",
        "def extract_frames(video_path, output_dir=\"frames\", frame_rate=1):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    video = VideoFileClip(video_path)\n",
        "    frames = []\n",
        "    for i, frame in enumerate(video.iter_frames(fps=frame_rate)):\n",
        "        frame_path = os.path.join(output_dir, f\"frame_{i}.jpg\")\n",
        "        img = Image.fromarray(frame)\n",
        "        img.save(frame_path)\n",
        "        frames.append(frame_path)\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Step 2: Generate captions for each frame\n",
        "def captions_helper(frames):\n",
        "    \"\"\"\n",
        "    Generate captions for a list of image frames using the BLIP model.\n",
        "    :param frames: List of paths to image frames.\n",
        "    :return: Combined caption string.\n",
        "    \"\"\"\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    captions = []\n",
        "    for frame_path in frames:\n",
        "        image = Image.open(frame_path).convert(\"RGB\")\n",
        "        inputs = processor(image, return_tensors=\"pt\")\n",
        "        output = model.generate(**inputs)\n",
        "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Clean up frames directory\n",
        "    for frame in frames:\n",
        "        os.remove(frame)\n",
        "\n",
        "    return \" \".join(captions)\n",
        "\n",
        "\n",
        "# Step 3: Main function\n",
        "def generate_caption(video_path):\n",
        "    print(\"Extracting frames from video...\")\n",
        "    frames = extract_frames(video_path)\n",
        "\n",
        "    print(\"Generating captions...\")\n",
        "    captions = captions_helper(frames)\n",
        "\n",
        "    print(\"\\nGenerated Captions:\")\n",
        "    return captions\n",
        "\n",
        "# Provide the path to the video file\n",
        "# video_path = \"gymnast.mp4\"\n",
        "# main(video_path)\n",
        "\n",
        "# def generate_caption(video_path):\n",
        "    # return [\"This is where the captions are supposed to go\"]\n",
        "\n",
        "def generate_video(captions,video_path):\n",
        "    return video_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_input_video(video_path):\n",
        "\n",
        "    caption = generate_caption(video_path)\n",
        "    output_video = generate_video(caption,video_path)\n",
        "\n",
        "    return video_path,output_video, caption\n",
        "\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn = process_input_video,\n",
        "    inputs = gr.Video(label=\"Upload Video\"),\n",
        "    outputs=[\n",
        "        gr.Video(label=\"Original Video\"),\n",
        "        gr.Video(label=\"Generated Video\"),\n",
        "        gr.Textbox(label = \"Generated Caption\")\n",
        "        ],\n",
        "    title=\"Video Generator\",\n",
        "    description=\"Upload video, get ai generated counterpart\",\n",
        ")\n",
        "\n",
        "interface.launch()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1XEksSvpaKdi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PKKEKI5baKQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QdX4WnrJaKOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary Vala"
      ],
      "metadata": {
        "id": "psV4pACEaLDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "TE6H-ijT93At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "import os\n",
        "import gradio as gr\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers.utils import export_to_video\n",
        "\n",
        "\n",
        "# Step 1: Extract frames from the video\n",
        "def extract_frames(video_path, output_dir=\"frames\", frame_rate=1):\n",
        "    \"\"\"\n",
        "    Extract frames from a video file at the specified frame rate (fps).\n",
        "    :param video_path: Path to the video file.\n",
        "    :param output_dir: Directory to save extracted frames.\n",
        "    :param frame_rate: Frames per second to extract.\n",
        "    :return: List of frame file paths.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    video = VideoFileClip(video_path)\n",
        "    frames = []\n",
        "    for i, frame in enumerate(video.iter_frames(fps=frame_rate)):\n",
        "        frame_path = os.path.join(output_dir, f\"frame_{i}.jpg\")\n",
        "        img = Image.fromarray(frame)\n",
        "        img.save(frame_path)\n",
        "        frames.append(frame_path)\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Step 2: Generate captions for each frame\n",
        "def generate_captions(frames):\n",
        "    \"\"\"\n",
        "    Generate captions for a list of image frames using the BLIP model.\n",
        "    :param frames: List of paths to image frames.\n",
        "    :return: Combined caption string.\n",
        "    \"\"\"\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    captions = []\n",
        "    for frame_path in frames:\n",
        "        image = Image.open(frame_path).convert(\"RGB\")\n",
        "        inputs = processor(image, return_tensors=\"pt\")\n",
        "        output = model.generate(**inputs)\n",
        "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Clean up frames directory\n",
        "    for frame in frames:\n",
        "        os.remove(frame)\n",
        "\n",
        "    return \" \".join(captions)\n",
        "\n",
        "# Step 3: Summarize the captions\n",
        "def summarize_captions(captions):\n",
        "    \"\"\"\n",
        "    Summarize the generated captions using a summarization model and format the output as a single, concise sentence.\n",
        "    :param captions: String containing all generated captions.\n",
        "    :return: Summarized caption string as a single concise sentence.\n",
        "    \"\"\"\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    summary = summarizer(captions, max_length=30, min_length=10, do_sample=False)  # Adjusting length for conciseness\n",
        "    summarized_text = summary[0][\"summary_text\"]\n",
        "\n",
        "    # Post-process to create a single short sentence\n",
        "    sentences = summarized_text.split(\". \")  # Split into individual sentences\n",
        "    short_summary = \" and \".join(sentences[:2])  # Combine only the first two main ideas\n",
        "    short_summary = short_summary.strip(\" .\") + \".\"  # Clean up trailing punctuation or spaces\n",
        "\n",
        "    return short_summary\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Main function\n",
        "def generate_caption(video_path):\n",
        "    print(\"Extracting frames from video...\")\n",
        "    frames = extract_frames(video_path)\n",
        "\n",
        "    print(\"Generating captions...\")\n",
        "    captions = summarize_captions(frames)\n",
        "\n",
        "    print(\"\\nGenerated Captions:\")\n",
        "    # print(captions)\n",
        "\n",
        "    print(\"\\nSummarizing Captions...\")\n",
        "    summary = summarize_captions(captions)\n",
        "    print(\"\\nSummary:\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Provide the path to the video file\n",
        "def generate_video(prompt):\n",
        "    pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    # memory optimization\n",
        "    pipe.enable_vae_slicing()\n",
        "\n",
        "    # prompt = \"A woman is sitting on the floor in a room and a man is walking with a cart.\"\n",
        "    video_frames = pipe(prompt, num_frames=64).frames[0]\n",
        "    video_path = export_to_video(video_frames)\n",
        "    print(\"Video saved to \",video_path)\n",
        "    return video_path\n",
        "\n",
        "\n",
        "\n",
        "def process_input_video(video_path):\n",
        "\n",
        "    caption = generate_caption(video_path)\n",
        "    output_video = generate_video(caption)\n",
        "\n",
        "    return video_path,output_video, caption\n",
        "\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn = process_input_video,\n",
        "    inputs = gr.Video(label=\"Upload Video\"),\n",
        "    outputs=[\n",
        "        gr.Video(label=\"Original Video\"),\n",
        "        gr.Video(label=\"Generated Video\"),\n",
        "        gr.Textbox(label = \"Generated Caption\")\n",
        "        ],\n",
        "    title=\"Video Generator\",\n",
        "    description=\"Upload video, get ai generated counterpart\",\n",
        ")\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "SaFuD_Z0aKGz",
        "outputId": "6f725f26-5ed2-4906-9547-3875d8450885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://67a71d1d17f53e6c79.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://67a71d1d17f53e6c79.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84-aaeW1aKD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v5NQbYCXaJ82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "myTfgR_D9vIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Step 1: Extract frames from the video\n",
        "def extract_frames(video_path, output_dir=\"frames\", frame_rate=1):\n",
        "    \"\"\"\n",
        "    Extract frames from a video file at the specified frame rate (fps).\n",
        "    :param video_path: Path to the video file.\n",
        "    :param output_dir: Directory to save extracted frames.\n",
        "    :param frame_rate: Frames per second to extract.\n",
        "    :return: List of frame file paths.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    video = VideoFileClip(video_path)\n",
        "    frames = []\n",
        "    for i, frame in enumerate(video.iter_frames(fps=frame_rate)):\n",
        "        frame_path = os.path.join(output_dir, f\"frame_{i}.jpg\")\n",
        "        img = Image.fromarray(frame)\n",
        "        img.save(frame_path)\n",
        "        frames.append(frame_path)\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Step 2: Generate captions for each frame\n",
        "def generate_captions(frames):\n",
        "    \"\"\"\n",
        "    Generate captions for a list of image frames using the BLIP model.\n",
        "    :param frames: List of paths to image frames.\n",
        "    :return: Combined caption string.\n",
        "    \"\"\"\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    captions = []\n",
        "    for frame_path in frames:\n",
        "        image = Image.open(frame_path).convert(\"RGB\")\n",
        "        inputs = processor(image, return_tensors=\"pt\")\n",
        "        output = model.generate(**inputs)\n",
        "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Clean up frames directory\n",
        "    for frame in frames:\n",
        "        os.remove(frame)\n",
        "\n",
        "    return \" \".join(captions)\n",
        "\n",
        "# Step 3: Summarize the captions\n",
        "def summarize_captions(captions):\n",
        "    \"\"\"\n",
        "    Summarize the generated captions using a summarization model and format the output as a single, concise sentence.\n",
        "    :param captions: String containing all generated captions.\n",
        "    :return: Summarized caption string as a single concise sentence.\n",
        "    \"\"\"\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    summary = summarizer(captions, max_length=30, min_length=10, do_sample=False)  # Adjusting length for conciseness\n",
        "    summarized_text = summary[0][\"summary_text\"]\n",
        "\n",
        "    # Post-process to create a single short sentence\n",
        "    sentences = summarized_text.split(\". \")  # Split into individual sentences\n",
        "    short_summary = \" and \".join(sentences[:2])  # Combine only the first two main ideas\n",
        "    short_summary = short_summary.strip(\" .\") + \".\"  # Clean up trailing punctuation or spaces\n",
        "\n",
        "    return short_summary\n",
        "\n",
        "# Main function\n",
        "def sumariser(video_path):\n",
        "    print(\"Extracting frames from video...\")\n",
        "    frames = extract_frames(video_path)\n",
        "\n",
        "    print(\"Generating captions...\")\n",
        "    captions = generate_captions(frames)\n",
        "\n",
        "    print(\"\\nGenerated Captions:\")\n",
        "    print(captions)\n",
        "\n",
        "    print(\"\\nSummarizing Captions...\")\n",
        "    summary = summarize_captions(captions)\n",
        "    print(\"\\nSummary:\")\n",
        "    return summary\n",
        "\n",
        "\n",
        "# caps = sumariser('/content/00X3U.mp4')\n",
        "# print(caps)\n",
        "\n",
        "\n",
        "def generate_new_video(prompt):\n",
        "    pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    # memory optimization\n",
        "    pipe.enable_vae_slicing()\n",
        "\n",
        "    # prompt = \"A woman is sitting on the floor in a room and a man is walking with a cart.\"\n",
        "    video_frames = pipe(prompt, num_frames=64).frames[0]\n",
        "    new_video_path = export_to_video(video_frames)\n",
        "    print(\"Video saved to \",new_video_path)\n",
        "    return new_video_path\n",
        "\n",
        "def generate_video(caption):\n",
        "    file_path = generate_new_video(caption)\n",
        "    return file_path\n",
        "\n",
        "def process_input_video(video_path):\n",
        "\n",
        "    caption = sumariser(video_path)\n",
        "    output_video = generate_video(caption)\n",
        "\n",
        "    return video_path,output_video, caption\n",
        "\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn = process_input_video,\n",
        "    inputs = gr.Video(label=\"Upload Video\"),\n",
        "    outputs=[\n",
        "        gr.Video(label=\"Original Video\"),\n",
        "        gr.Video(label=\"Generated Video\"),\n",
        "        gr.Textbox(label = \"Generated Caption\")\n",
        "        ],\n",
        "    title=\"Video Generator\",\n",
        "    description=\"Upload video, get ai generated counterpart\",\n",
        ")\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "-o0_IORQaJ62",
        "outputId": "e6a51944-d776-4d19-c6fb-09556e9c0219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9ab5de6e08f078eb2e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9ab5de6e08f078eb2e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Little test"
      ],
      "metadata": {
        "id": "QYJhZ3mFRJOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "import os\n",
        "import gradio as gr\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers.utils import export_to_video\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def extract_frames(video_path, output_dir=\"frames\", frame_rate=1):\n",
        "    \"\"\"\n",
        "    Extract frames from a video file at the specified frame rate (fps).\n",
        "    :param video_path: Path to the video file.\n",
        "    :param output_dir: Directory to save extracted frames.\n",
        "    :param frame_rate: Frames per second to extract.\n",
        "    :return: List of frame file paths.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    video = VideoFileClip(video_path)\n",
        "    frames = []\n",
        "    for i, frame in enumerate(video.iter_frames(fps=frame_rate)):\n",
        "        frame_path = os.path.join(output_dir, f\"frame_{i}.jpg\")\n",
        "        img = Image.fromarray(frame)\n",
        "        img.save(frame_path)\n",
        "        frames.append(frame_path)\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Step 2: Generate captions for each frame\n",
        "def generate_captions(frames):\n",
        "    \"\"\"\n",
        "    Generate captions for a list of image frames using the BLIP model.\n",
        "    :param frames: List of paths to image frames.\n",
        "    :return: Combined caption string.\n",
        "    \"\"\"\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    captions = []\n",
        "    for frame_path in frames:\n",
        "        image = Image.open(frame_path).convert(\"RGB\")\n",
        "        inputs = processor(image, return_tensors=\"pt\")\n",
        "        output = model.generate(**inputs)\n",
        "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Clean up frames directory\n",
        "    for frame in frames:\n",
        "        os.remove(frame)\n",
        "\n",
        "    return \" \".join(captions)\n",
        "\n",
        "# Step 3: Summarize the captions\n",
        "def summarize_captions(captions):\n",
        "    \"\"\"\n",
        "    Summarize the generated captions using a summarization model and format the output as a single, concise sentence.\n",
        "    :param captions: String containing all generated captions.\n",
        "    :return: Summarized caption string as a single concise sentence.\n",
        "    \"\"\"\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    summary = summarizer(captions, max_length=30, min_length=10, do_sample=False)  # Adjusting length for conciseness\n",
        "    summarized_text = summary[0][\"summary_text\"]\n",
        "\n",
        "    # Post-process to create a single short sentence\n",
        "    sentences = summarized_text.split(\". \")  # Split into individual sentences\n",
        "    short_summary = \" and \".join(sentences[:2])  # Combine only the first two main ideas\n",
        "    short_summary = short_summary.strip(\" .\") + \".\"  # Clean up trailing punctuation or spaces\n",
        "\n",
        "    return short_summary\n",
        "\n",
        "# Main function\n",
        "def sumariser(video_path):\n",
        "    print(\"Extracting frames from video...\")\n",
        "    frames = extract_frames(video_path)\n",
        "\n",
        "    print(\"Generating captions...\")\n",
        "    captions = generate_captions(frames)\n",
        "\n",
        "    print(\"\\nGenerated Captions:\")\n",
        "    print(captions)\n",
        "\n",
        "    print(\"\\nSummarizing Captions...\")\n",
        "    summary = summarize_captions(captions)\n",
        "    print(\"\\nSummary:\")\n",
        "    return summary\n",
        "\n",
        "\n",
        "def generate_video(prompt):\n",
        "    pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    # memory optimization\n",
        "    pipe.enable_vae_slicing()\n",
        "\n",
        "    # prompt = \"A woman is sitting on the floor in a room and a man is walking with a cart.\"\n",
        "    video_frames = pipe(prompt, num_frames=64).frames[0]\n",
        "    new_video_path = export_to_video(video_frames)\n",
        "    print(\"Video saved to \",new_video_path)\n",
        "    return new_video_path\n",
        "\n",
        "\n",
        "\n",
        "def move_and_rename_file(src_path, dest_dir, new_name):\n",
        "\n",
        "    # Ensure the destination directory exists\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "\n",
        "    # Construct the destination path with the new name\n",
        "    dest_path = os.path.join(dest_dir, new_name)\n",
        "\n",
        "    # Move and rename the file\n",
        "    shutil.move(src_path, dest_path)\n",
        "    print(f\"File moved and renamed to: {dest_path}\")\n",
        "\n",
        "    return dest_path\n",
        "\n",
        "def get_video(captions):\n",
        "    video_path_new = generate_video(captions)\n",
        "\n",
        "    src_file = video_path_new\n",
        "    dest_directory = \"/tmp/gradio/5c19e50d905bd3edd2bd9875abac89be9861a3f6a805d4dc8b78ab178eb3503b\"\n",
        "    new_filename = \"new_video.mp4\"\n",
        "\n",
        "    moved_file_path = move_and_rename_file(src_file, dest_directory, new_filename)\n",
        "\n",
        "    return moved_file_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_input_video(video_path):\n",
        "\n",
        "    caption = sumariser(video_path)\n",
        "    output_video = get_video(caption)\n",
        "\n",
        "    return video_path,output_video, caption\n",
        "\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn = process_input_video,\n",
        "    inputs = gr.Video(label=\"Upload Video\"),\n",
        "    outputs=[\n",
        "        gr.Video(label=\"Original Video\"),\n",
        "        gr.Video(label=\"Generated Video\"),\n",
        "        gr.Textbox(label = \"Generated Caption\")\n",
        "        ],\n",
        "    title=\"Video Generator\",\n",
        "    description=\"Upload video, get ai generated counterpart\",\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "BexHTh6XKxbW",
        "outputId": "dc3925d1-709e-4089-aa0f-ac8fb1fe75d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c08e549fe865556ee7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c08e549fe865556ee7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_video(prompt):\n",
        "    pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    # memory optimization\n",
        "    pipe.enable_vae_slicing()\n",
        "\n",
        "    # prompt = \"A woman is sitting on the floor in a room and a man is walking with a cart.\"\n",
        "    video_frames = pipe(prompt, num_frames=64).frames[0]\n",
        "    new_video_path = export_to_video(video_frames, output_path='/content/new_video.mp4')\n",
        "    print(\"Video saved to \",new_video_path)\n",
        "    return new_video_path\n",
        "\n",
        "\n",
        "\n",
        "generate_video(\"Spider man surfing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382,
          "referenced_widgets": [
            "38cf219f2e7844f1b58f3c919f9e100f",
            "4e7f4a133caa4c318ead64102d2644f7",
            "186773a73f09468eb5cc5b940aee9a34",
            "7e0836aa68c34f0988c25940f2d048ed",
            "c6e913e01f774674a4e237d30464ce2f",
            "dd2026efc17843c795a3dbc28f560205",
            "1b8e13f6d87740818e0ea4eaddb00775",
            "f4d832a8994947089d10dfcff63d971a",
            "a5df328136ff4aa29f349e46be82d6bf",
            "fa1274caed7b4c13834bba4b33f16958",
            "0ccf856bde8a4634b03da0c270684a89",
            "b67a3e75f3b24b268e62adcb060ec224",
            "77bd77175a9e4f5da434bec800ca059e",
            "122fd598de224df3aa65f014c40122d8",
            "0ca3784e4bb14310a129aee151d387e4",
            "da7e72baf30b4e4095b674877b318de0",
            "569f0d5a6b6a4c7bb21efb4dd6cfd2e9",
            "aadd3593520d4b249fa1e07305a4ad66",
            "0275213b87984070b0cd5ef64dde7981",
            "a0e5a3b15d864ab0a13e6b3a81be0a85",
            "a47e90e43d5c44dda2e3d8b6fe4f645d",
            "0999a45b51a24d789d7cc3b710e4aca7"
          ]
        },
        "id": "Qw-2JXXaOvSR",
        "outputId": "45120276-cb9c-4ee0-a94d-099371d07fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38cf219f2e7844f1b58f3c919f9e100f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b67a3e75f3b24b268e62adcb060ec224"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "export_to_video() got an unexpected keyword argument 'output_path'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8d9b6b9c037e>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgenerate_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spider man surfing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-8d9b6b9c037e>\u001b[0m in \u001b[0;36mgenerate_video\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# prompt = \"A woman is sitting on the floor in a room and a man is walking with a cart.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvideo_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnew_video_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport_to_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/new_video.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Video saved to \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_video_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_video_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: export_to_video() got an unexpected keyword argument 'output_path'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def move_and_rename_file(src_path, dest_dir, new_name):\n",
        "    \"\"\"\n",
        "    Move a file from one directory to another and rename it.\n",
        "    :param src_path: Path to the source file.\n",
        "    :param dest_dir: Destination directory.\n",
        "    :param new_name: New name for the file at the destination.\n",
        "    :return: Full path to the moved and renamed file.\n",
        "    \"\"\"\n",
        "    # Ensure the destination directory exists\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "\n",
        "    # Construct the destination path with the new name\n",
        "    dest_path = os.path.join(dest_dir, new_name)\n",
        "\n",
        "    # Move and rename the file\n",
        "    shutil.move(src_path, dest_path)\n",
        "    print(f\"File moved and renamed to: {dest_path}\")\n",
        "\n",
        "    return dest_path\n",
        "\n",
        "# Example usage\n",
        "src_file = \"/tmp/tmpl99dzmqv.mp4\"\n",
        "dest_directory = \"/content\"\n",
        "new_filename = \"new_video.mp4\"\n",
        "\n",
        "moved_file_path = move_and_rename_file(src_file, dest_directory, new_filename)\n",
        "print(f\"Moved file path: {moved_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZWr576PPVd8",
        "outputId": "4c0c1ce9-1de8-4bfe-d35b-748b4cd3e36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File moved and renamed to: /content/new_video.mp4\n",
            "Moved file path: /content/new_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Code"
      ],
      "metadata": {
        "id": "HSA6HdcjZ2ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "import os\n",
        "import gradio as gr\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers.utils import export_to_video\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def extract_frames(video_path, output_dir=\"frames\", frame_rate=1):\n",
        "    \"\"\"\n",
        "    Extract frames from a video file at the specified frame rate (fps).\n",
        "    :param video_path: Path to the video file.\n",
        "    :param output_dir: Directory to save extracted frames.\n",
        "    :param frame_rate: Frames per second to extract.\n",
        "    :return: List of frame file paths.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    video = VideoFileClip(video_path)\n",
        "    frames = []\n",
        "    for i, frame in enumerate(video.iter_frames(fps=frame_rate)):\n",
        "        frame_path = os.path.join(output_dir, f\"frame_{i}.jpg\")\n",
        "        img = Image.fromarray(frame)\n",
        "        img.save(frame_path)\n",
        "        frames.append(frame_path)\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Step 2: Generate captions for each frame\n",
        "def generate_captions(frames):\n",
        "    \"\"\"\n",
        "    Generate captions for a list of image frames using the BLIP model.\n",
        "    :param frames: List of paths to image frames.\n",
        "    :return: Combined caption string.\n",
        "    \"\"\"\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    captions = []\n",
        "    for frame_path in frames:\n",
        "        image = Image.open(frame_path).convert(\"RGB\")\n",
        "        inputs = processor(image, return_tensors=\"pt\")\n",
        "        output = model.generate(**inputs)\n",
        "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Clean up frames directory\n",
        "    for frame in frames:\n",
        "        os.remove(frame)\n",
        "\n",
        "    return \" \".join(captions)\n",
        "\n",
        "# Step 3: Summarize the captions\n",
        "def summarize_captions(captions):\n",
        "    \"\"\"\n",
        "    Summarize the generated captions using a summarization model and format the output as a single, concise sentence.\n",
        "    :param captions: String containing all generated captions.\n",
        "    :return: Summarized caption string as a single concise sentence.\n",
        "    \"\"\"\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    summary = summarizer(captions, max_length=30, min_length=10, do_sample=False)  # Adjusting length for conciseness\n",
        "    summarized_text = summary[0][\"summary_text\"]\n",
        "\n",
        "    # Post-process to create a single short sentence\n",
        "    sentences = summarized_text.split(\". \")  # Split into individual sentences\n",
        "    short_summary = \" and \".join(sentences[:2])  # Combine only the first two main ideas\n",
        "    short_summary = short_summary.strip(\" .\") + \".\"  # Clean up trailing punctuation or spaces\n",
        "\n",
        "    return short_summary\n",
        "\n",
        "# Main function\n",
        "def sumariser(video_path):\n",
        "    print(\"Extracting frames from video...\")\n",
        "    frames = extract_frames(video_path)\n",
        "\n",
        "    print(\"Generating captions...\")\n",
        "    captions = generate_captions(frames)\n",
        "\n",
        "    print(\"\\nGenerated Captions:\")\n",
        "    print(captions)\n",
        "\n",
        "    print(\"\\nSummarizing Captions...\")\n",
        "    summary = summarize_captions(captions)\n",
        "    print(\"\\nSummary:\")\n",
        "    return summary\n",
        "\n",
        "\n",
        "def generate_video(prompt):\n",
        "    pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    # memory optimization\n",
        "    pipe.enable_vae_slicing()\n",
        "\n",
        "    # prompt = \"A woman is sitting on the floor in a room and a man is walking with a cart.\"\n",
        "    video_frames = pipe(prompt, num_frames=64).frames[0]\n",
        "    new_video_path = export_to_video(video_frames)\n",
        "    print(\"Video saved to \",new_video_path)\n",
        "    return new_video_path\n",
        "\n",
        "\n",
        "\n",
        "def move_and_rename_file(src_path, dest_dir, new_name):\n",
        "  # Ensure the destination directory exists\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "\n",
        "    # Construct the destination path with the new name\n",
        "    dest_path = os.path.join(dest_dir, new_name)\n",
        "\n",
        "    # Move and rename the file\n",
        "    shutil.move(src_path, dest_path)\n",
        "    print(f\"File moved and renamed to: {dest_path}\")\n",
        "\n",
        "    return dest_path\n",
        "\n",
        "def get_video(captions):\n",
        "    video_path_new = generate_video(captions)\n",
        "\n",
        "    src_file = video_path_new\n",
        "    dest_directory = \"/tmp/gradio/5c19e50d905bd3edd2bd9875abac89be9861a3f6a805d4dc8b78ab178eb3503b\"\n",
        "    new_filename = \"new_video.mp4\"\n",
        "\n",
        "    moved_file_path = move_and_rename_file(src_file, dest_directory, new_filename)\n",
        "\n",
        "    return moved_file_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_input_video(video_path):\n",
        "\n",
        "    caption = sumariser(video_path)\n",
        "    output_video = get_video(caption)\n",
        "\n",
        "    return video_path,output_video, caption\n",
        "\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn = process_input_video,\n",
        "    inputs = gr.Video(label=\"Upload Video\"),\n",
        "    outputs=[\n",
        "        gr.Video(label=\"Original Video\"),\n",
        "        gr.Video(label=\"Generated Video\"),\n",
        "        gr.Textbox(label = \"Generated Caption\")\n",
        "        ],\n",
        "    title=\"Video Generator\",\n",
        "    description=\"Upload video, get ai generated counterpart\",\n",
        ")\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "sPbeXHHpZ8Nx",
        "outputId": "f560d546-2341-48ca-f254-713a2d333450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7ad338d8d3d2981661.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7ad338d8d3d2981661.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}