# Install required libraries
!pip install transformers
!pip install PyPDF2
!pip install nltk

# Import necessary libraries
from transformers import AutoModel, AutoTokenizer
import requests
from bs4 import BeautifulSoup
import re
import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import sent_tokenize
import numpy as np
import pandas as pd
import torch
import PyPDF2
import itertools

import PyPDF2
import re
import nltk
from nltk.tokenize import sent_tokenize

# Define preprocess_text function
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    tokens = nltk.word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    stop_words = nltk.corpus.stopwords.words('english')
    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]
    text = ' '.join(filtered_tokens)
    return text

# Function to extract text from PDF in sentences
def extract_text_sentences(pdf_file_path):
    with open(pdf_file_path, "rb") as pdf_file:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        ipc_text_sentences = []
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            sentences = sent_tokenize(page_text)
            ipc_text_sentences.extend(sentences)
    return ipc_text_sentences

# Path to the PDF file
file_path = "ipc_corpus.pdf"  # Replace with the correct path to your "ipc_corpus.pdf" file

# Extract text from the PDF in sentences
ipc_sentences = extract_text_sentences(file_path)

# Preprocess sentences for analysis
preprocessed_sentences = [preprocess_text(sentence) for sentence in ipc_sentences]

# Create a DataFrame with preprocessed sentences
ipc_df_sentences = pd.DataFrame({"preprocessed_sentence": preprocessed_sentences})

# Function to extract IPC data from sentences
# Function to extract IPC data from sentences with POS tagging
def get_ipc_data_from_sentences(ipc_sentences):
    section_pattern = re.compile(r"(\d+\.\s.+?)\.\s")
    description_pattern = re.compile(r"\n(.+?)\n-")
    penalty_pattern = re.compile(r"\n(.+?)\n\.")

    sections = []
    descriptions = []
    penalties = []
    pos_tags = []  # POS tags for descriptions

    for sentence in ipc_sentences:
        section_match = re.search(section_pattern, sentence)
        if section_match:
            sections.append(section_match.group(1))
        else:
            sections.append(None)
            print("No section found in:", sentence)

        description_match = re.search(description_pattern, sentence)
        if description_match:
            description = description_match.group(1)
            descriptions.append(description)

            # POS tagging for descriptions
            tokens = nltk.word_tokenize(description)
            tagged_tokens = nltk.pos_tag(tokens)
            pos_tags.append(tagged_tokens)
        else:
            descriptions.append(None)
            pos_tags.append(None)
            print("No description found in:", sentence)

        penalty_match = re.search(penalty_pattern, sentence)
        if penalty_match:
            penalties.append(penalty_match.group(1))
        else:
            penalties.append(None)
            print("No penalty found in:", sentence)

    ipc_df = pd.DataFrame({"section": sections, "description": descriptions, "penalty": penalties, "pos_tags": pos_tags})
    return ipc_df

# Create DataFrame with IPC data extracted from sentences
ipc_df = get_ipc_data_from_sentences(ipc_sentences)

# Define analyze_prompt function
from itertools import chain

def analyze_prompt(text, ipc_data):
  try:
    # Preprocess the text
    text = preprocess_text(text)

    # Get POS tags for words
    for index, row in ipc_data.iterrows():
      pos_tags = row['pos_tags']

    # Extract verbs, prioritizing VB tags and allowing fallback to NN if no verbs found
    verbs = [word for word, tag in nltk.pos_tag(nltk.word_tokenize(text)) if tag.startswith("VB")]
    if not verbs:
      verbs = [word for word, tag in nltk.pos_tag(nltk.word_tokenize(text)) if tag.startswith("VB") or tag.startswith("NN")]

    # Make verbs unique
    verbs = set(verbs)

    # Print processed text and identified verbs for debugging
    print("Processed Text:", text)
    print("Identified Verbs:", verbs)

    # Initialize set of legal verbs found in IPC descriptions
    legal_verbs = set()

    # Loop through each identified verb
    for verb in verbs:
      try:
        # Handle different tag formats: single element (word only) or tuple (word, tag)
        if isinstance(pos_tags, str):
          # Single-word tag: Check direct match for verb, "theft", and "Grand Theft Auto"
          matching_words = [pos_tags] if pos_tags == verb or pos_tags.lower() == "theft" or pos_tags.lower() == "grand theft auto" else []
        elif pos_tags:
          # Tuple list: Use chain.from_iterable for flattening and verb match
          matching_words = [word for word, tag in chain.from_iterable(pos_tags) if tag.startswith("VB") and word == verb]
          # Filter out empty values
          matching_words = [word for word in matching_words if word]
        else:
          matching_words = []

        # If matching word found, add verb to legal verbs and break to next row
        if matching_words:
          legal_verbs.add(verb)
          break

      except Exception as e:
        print(f"Error in matching for '{verb}': {e}")

    # Initialize list of identified violations
    violations = []

    # Loop through each legal verb found
    for legal_verb in legal_verbs:
      try:
        # Create regex pattern for matching verb and its relevant synonyms
        # Modify synonym selection based on your specific IPC data and needs
        regex = r"\b({})\b|{}".format(legal_verb, "|".join(synset.lemma_names() for synset in nltk.corpus.wordnet.synsets(legal_verb) if "theft" in synset.definition()))

        # Find all matches of the pattern in IPC descriptions
        matches = re.finditer(regex, str(ipc_data["description"]))

        # Loop through each match
        for match in matches:
          # Extract relevant information from matched description
          section = ipc_data["section"][match.start()] if match.start() < len(ipc_data["section"]) else None
          description = ipc_data["description"][match.start()] if match.start() < len(ipc_data["description"]) else None
          penalty = ipc_data["penalty"][match.start()] if match.start() < len(ipc_data["penalty"]) else None

          # Append extracted information to list of violations
          violations.append((section, description, penalty))

      except Exception as e:
        print(f"Error in matching for '{legal_verb}': {e}")

  except Exception as e:
    print(f"Error while analyzing prompt: {e}")
    return []

  return violations

# Define generate_explanations function
def generate_explanations(violations):
    explanations = []
    for violation in violations:
        section, description, penalty = violation
        explanation = f"According to IPC section {section}, {description}. The theoretical penalty for this violation is {penalty}."
        explanations.append(explanation)
    return explanations

# Define present_output function
def present_output(explanations):
    if explanations:
        output = "\n".join(explanations)
        print(output)
    else:
        print("No IPC violations were found in the prompt.")

# Get user prompt
prompt = input("Please enter a prompt describing an action or a crime: ")

# Analyze the prompt and generate explanations
violations = analyze_prompt(prompt, ipc_df)

# Generate detailed explanations for each identified violation
explanations = generate_explanations(violations)

# Present the final output
present_output(explanations)
