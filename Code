# Install required libraries
!pip install transformers
!pip install PyPDF2
!pip install nltk

# Import necessary libraries
from transformers import AutoModel, AutoTokenizer
import requests
from bs4 import BeautifulSoup
import re
import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import sent_tokenize
import numpy as np
import pandas as pd
import torch
import PyPDF2

# Define preprocess_text function
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    tokens = nltk.word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    stop_words = nltk.corpus.stopwords.words('english')
    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]
    text = ' '.join(filtered_tokens)
    return text

# Function to extract text from PDF in sentences
def extract_text_sentences(pdf_file_path):
    with open(pdf_file_path, "rb") as pdf_file:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        ipc_text_sentences = []
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            sentences = sent_tokenize(page_text)
            ipc_text_sentences.extend(sentences)
    return ipc_text_sentences

# URL of the PDF file
url = "https://www.indiacode.nic.in/bitstream/123456789/4219/1/THE-INDIAN-PENAL-CODE-1860.pdf"

# Download the PDF file
file_path = "/content/aA1860-45.pdf"
with open(file_path, "wb") as file:
    response = requests.get(url)
    file.write(response.content)

# Extract text from the PDF in sentences
ipc_sentences = extract_text_sentences(file_path)

# Preprocess sentences for analysis
preprocessed_sentences = [preprocess_text(sentence) for sentence in ipc_sentences]

# Create a DataFrame with preprocessed sentences
ipc_df_sentences = pd.DataFrame({"preprocessed_sentence": preprocessed_sentences})

# Function to extract IPC data from sentences
# Function to extract IPC data from sentences with POS tagging
def get_ipc_data_from_sentences(ipc_sentences):
    section_pattern = re.compile(r"(\d+\.\s.+?)\.\s")
    description_pattern = re.compile(r"\n(.+?)\n-")
    penalty_pattern = re.compile(r"\n(.+?)\n\.")

    sections = []
    descriptions = []
    penalties = []
    pos_tags = []  # POS tags for descriptions

    for sentence in ipc_sentences:
        section_match = re.search(section_pattern, sentence)
        if section_match:
            sections.append(section_match.group(1))
        else:
            sections.append(None)
            print("No section found in:", sentence)

        description_match = re.search(description_pattern, sentence)
        if description_match:
            description = description_match.group(1)
            descriptions.append(description)

            # POS tagging for descriptions
            tokens = nltk.word_tokenize(description)
            tagged_tokens = nltk.pos_tag(tokens)
            pos_tags.append(tagged_tokens)
        else:
            descriptions.append(None)
            pos_tags.append(None)
            print("No description found in:", sentence)

        penalty_match = re.search(penalty_pattern, sentence)
        if penalty_match:
            penalties.append(penalty_match.group(1))
        else:
            penalties.append(None)
            print("No penalty found in:", sentence)

    ipc_df = pd.DataFrame({"section": sections, "description": descriptions, "penalty": penalties, "pos_tags": pos_tags})
    return ipc_df

# Create DataFrame with IPC data extracted from sentences
ipc_df = get_ipc_data_from_sentences(ipc_sentences)

# Define analyze_prompt function
def analyze_prompt(text, ipc_data):
    try:
        text = preprocess_text(text)
        tagged_words = nltk.pos_tag(nltk.word_tokenize(text))

        verbs = [word for word, tag in tagged_words if tag.startswith("VB")]

        if not verbs:
            verbs = [word for word, tag in tagged_words if tag.startswith("VB") or tag.startswith("NN")]

        verbs = set(verbs)

        print("Processed Text:", text)
        print("Identified Verbs:", verbs)

        legal_verbs = set()
        for verb in verbs:
            try:
                for index, row in ipc_data.iterrows():
                    pos_tags = row['pos_tags']
                    if pos_tags:
                        matching_words = [word for word, tag in pos_tags if tag.startswith("VB") and word == verb]
                        if matching_words:
                            legal_verbs.add(verb)
                            break
                else:
                    legal_verbs.add(verb)
            except Exception as e:
                print(f"Error in matching for '{verb}': {e}")

        violations = []
        for legal_verb in legal_verbs:
            try:
                regex = r"\b({})\b|{}".format(legal_verb, "|".join(synset.lemma_names() for synset in nltk.corpus.wordnet.synsets(legal_verb)))
                matches = re.finditer(regex, str(ipc_data["description"]))
                for match in matches:
                    section = ipc_data["section"][match.start()] if match.start() < len(ipc_data["section"]) else None
                    description = ipc_data["description"][match.start()] if match.start() < len(ipc_data["description"]) else None
                    penalty = ipc_data["penalty"][match.start()] if match.start() < len(ipc_data["penalty"]) else None
                    violations.append((section, description, penalty))
            except Exception as e:
                print(f"Error in matching for '{legal_verb}': {e}")

    except Exception as e:
        print(f"Error while analyzing prompt: {e}")
        return []

    return violations


# Define generate_explanations function
def generate_explanations(violations):
    explanations = []
    for violation in violations:
        section, description, penalty = violation
        explanation = f"According to IPC section {section}, {description}. The theoretical penalty for this violation is {penalty}."
        explanations.append(explanation)
    return explanations

# Define present_output function
def present_output(explanations):
    if explanations:
        output = "\n".join(explanations)
        print(output)
    else:
        print("No IPC violations were found in the prompt.")

# Get user prompt
prompt = input("Please enter a prompt describing an action or a crime: ")

# Analyze the prompt and generate explanations
violations = analyze_prompt(prompt, ipc_df)

# Generate detailed explanations for each identified violation
explanations = generate_explanations(violations)

# Present the final output
present_output(explanations)
